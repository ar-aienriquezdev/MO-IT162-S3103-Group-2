{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinMarks — Dataset Preprocessing (Cross‑Platform Paths)\n",
    "\n",
    "This notebook loads and cleans three CSV inputs, engineers features, merges them, and writes **`merged_cleaned_dataset.csv`**.\n",
    "It uses a **generic path resolver** that works across Windows, macOS, and Linux without any user‑specific paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & Settings ---\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved paths:\n",
      "  demographics: /Users/arenriquez1/Downloads/customer_demographics_contaminated.csv\n",
      "  transactions: /Users/arenriquez1/Downloads/customer_transactions_contaminated.csv\n",
      "  social      : /Users/arenriquez1/Downloads/social_media_interactions_contaminated.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Robust path resolver (generic) ---\n",
    "# You can optionally set RAW_DIR as an environment variable to point to a folder to search first.\n",
    "#   e.g., in a terminal before launching Jupyter:  export RAW_DIR=/path/to/data\n",
    "# Otherwise the resolver will search common locations.\n",
    "\n",
    "RAW_DIR = os.environ.get(\"RAW_DIR\")\n",
    "\n",
    "SEARCH_DIRS = []\n",
    "if RAW_DIR:\n",
    "    SEARCH_DIRS.append(Path(RAW_DIR))\n",
    "\n",
    "# Common cross‑platform locations to search (no user-specific paths)\n",
    "SEARCH_DIRS += [\n",
    "    Path.cwd(),                          # current notebook directory\n",
    "    Path.cwd().parent,                   # project root (one level up)\n",
    "    Path.cwd() / \"data\",                 # ./data\n",
    "    Path.home(),                         # home directory\n",
    "    Path.home() / \"Downloads\",           # Downloads\n",
    "]\n",
    "\n",
    "FILENAMES = {\n",
    "    \"demographics\": \"customer_demographics_contaminated.csv\",\n",
    "    \"transactions\": \"customer_transactions_contaminated.csv\",\n",
    "    \"social\":       \"social_media_interactions_contaminated.csv\",\n",
    "}\n",
    "\n",
    "def resolve_path(name: str) -> Path:\n",
    "    fname = FILENAMES[name]\n",
    "    # 1) direct check\n",
    "    for d in SEARCH_DIRS:\n",
    "        p = d / fname\n",
    "        if p.exists():\n",
    "            return p\n",
    "    # 2) shallow recursive search (bounded)\n",
    "    for d in SEARCH_DIRS:\n",
    "        try:\n",
    "            matches = list(d.glob(f\"**/{fname}\"))\n",
    "        except Exception:\n",
    "            matches = []\n",
    "        if matches:\n",
    "            return matches[0]\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find '{fname}'.\\n\"\n",
    "        \"Tip: Place the CSVs beside this notebook, in a ./data folder, in your Downloads, \"\n",
    "        \"or set environment variable RAW_DIR to the directory containing the files.\"\n",
    "    )\n",
    "\n",
    "demographics_path = resolve_path(\"demographics\")\n",
    "transactions_path = resolve_path(\"transactions\")\n",
    "social_path       = resolve_path(\"social\")\n",
    "\n",
    "print(\"Resolved paths:\")\n",
    "print(\"  demographics:\", demographics_path)\n",
    "print(\"  transactions:\", transactions_path)\n",
    "print(\"  social      :\", social_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load ---\n",
    "demographics = pd.read_csv(demographics_path)\n",
    "transactions = pd.read_csv(transactions_path)\n",
    "social       = pd.read_csv(social_path)\n",
    "\n",
    "# Standardize id column name across frames early\n",
    "for df in (demographics, transactions, social):\n",
    "    if \"CustomerID\" in df.columns:\n",
    "        df[\"CustomerID\"] = df[\"CustomerID\"].astype(\"string\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Basic Cleaning ---\n",
    "# Drop exact duplicates\n",
    "demographics = demographics.drop_duplicates()\n",
    "transactions = transactions.drop_duplicates()\n",
    "social       = social.drop_duplicates()\n",
    "\n",
    "# Coerce selected numeric columns if present\n",
    "if \"Age\" in demographics.columns:\n",
    "    demographics[\"Age\"] = pd.to_numeric(demographics[\"Age\"], errors=\"coerce\")\n",
    "\n",
    "if \"Amount\" in transactions.columns:\n",
    "    transactions[\"Amount\"] = pd.to_numeric(transactions[\"Amount\"], errors=\"coerce\")\n",
    "\n",
    "# Handle missing values (lightweight defaults; adjust as needed)\n",
    "if \"gender\" in demographics.columns:\n",
    "    demographics[\"gender\"] = demographics[\"gender\"].fillna(\"Unknown\")\n",
    "if \"Age\" in demographics.columns:\n",
    "    demographics[\"Age\"] = demographics[\"Age\"].fillna(demographics[\"Age\"].median())\n",
    "\n",
    "if \"Amount\" in transactions.columns:\n",
    "    transactions[\"Amount\"] = transactions[\"Amount\"].fillna(transactions[\"Amount\"].median())\n",
    "\n",
    "# For social metrics, fill numeric nulls with 0\n",
    "num_cols_social = social.select_dtypes(include=\"number\").columns\n",
    "social[num_cols_social] = social[num_cols_social].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fq/wg6mt70d12361qx6fnd63pjc0000gn/T/ipykernel_20975/169398591.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  social[\"engagement_score\"] = social_numeric.sum(axis=1)\n"
     ]
    }
   ],
   "source": [
    "# --- Feature Engineering ---\n",
    "# Transactions: aggregate spend metrics per customer\n",
    "if \"Amount\" in transactions.columns:\n",
    "    transactions[\"Amount\"] = pd.to_numeric(transactions[\"Amount\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "transaction_features = (\n",
    "    transactions.groupby(\"CustomerID\", observed=True)[\"Amount\"]\n",
    "    .agg(total_spent=\"sum\", avg_spent=\"mean\", num_transactions=\"count\",\n",
    "         max_transaction=\"max\", min_transaction=\"min\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Social: compute engagement as sum of numeric columns (excluding CustomerID)\n",
    "social_numeric = social.drop(columns=[c for c in [\"CustomerID\"] if c in social.columns]).select_dtypes(include=\"number\")\n",
    "social[\"engagement_score\"] = social_numeric.sum(axis=1)\n",
    "\n",
    "agg_dict = {col: \"sum\" for col in social_numeric.columns}\n",
    "agg_dict[\"engagement_score\"] = \"sum\"\n",
    "\n",
    "social_features = social.groupby(\"CustomerID\", observed=True).agg(agg_dict).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset shape: (1192, 12)\n",
      "Columns: ['customer_id', 'age', 'gender', 'location', 'incomelevel', 'signupdate', 'total_spent', 'avg_spent', 'num_transactions', 'max_transaction', 'min_transaction', 'engagement_score']\n",
      "Unique customers: 1181\n"
     ]
    }
   ],
   "source": [
    "# --- Merge ---\n",
    "# Normalize key name\n",
    "for df in (demographics, transaction_features, social_features):\n",
    "    df.rename(columns={\"CustomerID\": \"customer_id\"}, inplace=True)\n",
    "\n",
    "merged = demographics.merge(transaction_features, on=\"customer_id\", how=\"inner\") \\\n",
    "                     .merge(social_features, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "# Lowercase columns\n",
    "merged.columns = merged.columns.str.lower()\n",
    "\n",
    "print(\"Merged dataset shape:\", merged.shape)\n",
    "print(\"Columns:\", merged.columns.tolist())\n",
    "print(\"Unique customers:\", merged[\"customer_id\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/arenriquez1/NetBeansProjects/MO-IT162-S3103-Group-2/Milestone 2: Data Visualization on Machine Learning Solution Project/merged_cleaned_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Save ---\n",
    "out_path = Path(\"merged_cleaned_dataset.csv\")\n",
    "merged.to_csv(out_path, index=False)\n",
    "print(f\"Saved: {out_path.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
